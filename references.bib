
@article{garnelo2018,
	title = {Conditional {Neural} {Processes}},
	url = {http://arxiv.org/abs/1807.01613},
	abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
	urldate = {2021-06-10},
	journal = {arXiv:1807.01613 [cs, stat]},
	author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J. and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01613},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zinzan/Documents/Apps/Zotero/storage/F9WHZS4E/Garnelo et al. - 2018 - Conditional Neural Processes.pdf:application/pdf},
}

@article{garnelo2018a,
	title = {Neural {Processes}},
	url = {http://arxiv.org/abs/1807.01622},
	abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
	urldate = {2021-06-10},
	journal = {arXiv:1807.01622 [cs, stat]},
	author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01622},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zinzan/Documents/Apps/Zotero/storage/K3ML4XHF/Garnelo et al. - 2018 - Neural Processes.pdf:application/pdf},
}

@inproceedings{kim2018,
	title = {Attentive {Neural} {Processes}},
	url = {https://openreview.net/forum?id=SkE6PjC9KX},
	abstract = {A model for regression that learns conditional distributions of a stochastic process, by incorporating attention into Neural Processes.},
	language = {en},
	urldate = {2021-06-10},
	author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/zinzan/Documents/Apps/Zotero/storage/RX84LNI3/Kim et al. - 2018 - Attentive Neural Processes.pdf:application/pdf},
}

@inproceedings{gordon2019,
	title = {Convolutional {Conditional} {Neural} {Processes}},
	url = {https://openreview.net/forum?id=Skey4eBYPS},
	abstract = {We extend deep sets to functional embeddings and Neural Processes to include translation equivariant members},
	language = {en},
	urldate = {2021-06-10},
	author = {Gordon, Jonathan and Bruinsma, Wessel P. and Foong, Andrew Y. K. and Requeima, James and Dubois, Yann and Turner, Richard E.},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/Users/zinzan/Documents/Apps/Zotero/storage/IJND94NW/Gordon et al. - 2019 - Convolutional Conditional Neural Processes.pdf:application/pdf},
}

@article{foong2020,
	title = {Meta-{Learning} {Stationary} {Stochastic} {Process} {Prediction} with {Convolutional} {Neural} {Processes}},
	url = {http://arxiv.org/abs/2007.01332},
	abstract = {Stationary stochastic processes (SPs) are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive SPs, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (ConvNP), which endows Neural Processes (NPs) with translation equivariance and extends convolutional conditional NPs to allow for dependencies in the predictive distribution. The latter enables ConvNPs to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard ELBO objective in NPs, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of ConvNPs on 1D regression, image completion, and various tasks with real-world spatio-temporal data.},
	urldate = {2021-06-10},
	journal = {arXiv:2007.01332 [cs, stat]},
	author = {Foong, Andrew Y. K. and Bruinsma, Wessel P. and Gordon, Jonathan and Dubois, Yann and Requeima, James and Turner, Richard E.},
	month = nov,
	year = {2020},
	note = {arXiv: 2007.01332},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zinzan/Documents/Apps/Zotero/storage/QWD2LB74/Foong et al. - 2020 - Meta-Learning Stationary Stochastic Process Predic.pdf:application/pdf},
}

@inproceedings{bruinsma2020,
	title = {The {Gaussian} {Neural} {Process}},
	url = {https://openreview.net/forum?id=rzsDn7Vzxf},
	abstract = {Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the...},
	language = {en},
	urldate = {2021-06-10},
	author = {Bruinsma, Wessel and Requeima, James and Foong, Andrew Y. K. and Gordon, Jonathan and Turner, Richard E.},
	month = nov,
	year = {2020},
	file = {Full Text PDF:/Users/zinzan/Documents/Apps/Zotero/storage/HHR8NZV5/Bruinsma et al. - 2020 - The Gaussian Neural Process.pdf:application/pdf},
}

@techreport{vaughan2021,
	type = {preprint},
	title = {Convolutional conditional neural processes for local climatedownscaling},
	url = {https://gmd.copernicus.org/preprints/gmd-2020-420/},
	abstract = {A new model is presented for multisite statistical downscaling of temperature and precipitation using convolutional conditional neural processes (convCNPs). ConvCNPs are a recently developed class of models that allow deep learning techniques to be applied to off-the-grid spatio-temporal data. This model has a substantial advantage over existing downscaling methods 5 in that the trained model can be used to generate multisite predictions at an arbitrary set of locations, regardless of the availability of training data. The convCNP model is shown to outperform an ensemble of existing downscaling techniques over Europe for both temperature and precipitation taken from the VALUE intercomparison project. The model also outperforms an approach that uses Gaussian processes to interpolate single-site downscaling models at unseen locations. Importantly, substantial improvement is seen in the representation of extreme precipitation events. These results indicate that the convCNP is a 10 robust downscaling model suitable for generating localised projections for use in climate impact studies, and motivates further research into applications of deep learning techniques in statistical downscaling.},
	language = {en},
	urldate = {2021-10-08},
	institution = {Climate and Earth system modeling},
	author = {Vaughan, Anna and Tebbutt, Will and Hosking, J. Scott and Turner, Richard E.},
	month = mar,
	year = {2021},
	doi = {10.5194/gmd-2020-420},
	file = {Vaughan et al. - 2021 - Convolutional conditional neural processes for loc.pdf:/Users/zinzan/Documents/Apps/Zotero/storage/5IKCBN3V/Vaughan et al. - 2021 - Convolutional conditional neural processes for loc.pdf:application/pdf},
}

@inproceedings{requeima2019,
	title = {Fast and {Flexible} {Multi}-{Task} {Classification} using {Conditional} {Neural} {Adaptive} {Processes}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/1138d90ef0a0848a542e57d1595f58ea-Abstract.html},
	urldate = {2021-10-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Requeima, James and Gordon, Jonathan and Bronskill, John and Nowozin, Sebastian and Turner, Richard E},
	year = {2019},
	file = {Full Text PDF:/Users/zinzan/Documents/Apps/Zotero/storage/BRLNAUPW/Requeima et al. - 2019 - Fast and Flexible Multi-Task Classification using .pdf:application/pdf},
}

@book{dubois2020,
	title = {Neural {Process} {Family}},
	url = {https://yanndubs.github.io/Neural-Process-Family/},
	urldate = {2021-10-15},
	author = {Dubois, Yann and Gordon, Jonathan and Foong, Andrew Y. K.},
	month = sep,
	year = {2020},
}

@article{louizos,
	title = {The {Functional} {Neural} {Process}},
	abstract = {We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they deﬁne a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and image classiﬁcation and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates.},
	language = {en},
	author = {Louizos, Christos and Shi, Xiahan and Schutte, Klamer and Welling, Max},
	pages = {12},
	file = {Louizos et al. - The Functional Neural Process.pdf:/Users/zinzan/Documents/Apps/Zotero/storage/MAK6M4E6/Louizos et al. - The Functional Neural Process.pdf:application/pdf},
}

@inproceedings{anonymous2021,
	title = {Practical {Conditional} {Neural} {Process} {Via} {Tractable} {Dependent} {Predictions}},
	url = {https://openreview.net/forum?id=3pugbNqOh5m},
	abstract = {Conditional Neural Processes (CNPs; Garnelo et al., 2018) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle...},
	language = {en},
	urldate = {2021-10-21},
	author = {Anonymous},
	month = sep,
	year = {2021},
	file = {Full Text PDF:/Users/zinzan/Documents/Apps/Zotero/storage/RLIHS5UF/Anonymous - 2021 - Practical Conditional Neural Process Via Tractable.pdf:application/pdf},
}
